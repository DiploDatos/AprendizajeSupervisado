{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Recommendation Systems\n",
    "\n",
    "The code is from https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Building%20Recommender%20System%20with%20Surprise.ipynb. Explained in detail here: https://towardsdatascience.com/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise.accuracy import rmse\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv('./demo_11_dataset/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "user.columns = ['userID', 'Location', 'Age']\n",
    "rating = pd.read_csv('./demo_11_dataset/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "rating.columns = ['userID', 'ISBN', 'bookRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(user, rating, on='userID', how='inner')\n",
    "df.drop(['Location', 'Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset shape: {}'.format(df.shape))\n",
    "print('-Dataset examples-')\n",
    "print(df.iloc[::200000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "### Ratings Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "data = df['bookRating'].value_counts().sort_index(ascending=False)\n",
    "trace = go.Bar(x = data.index,\n",
    "               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n",
    "               textposition = 'auto',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               y = data.values,\n",
    "               )\n",
    "# Create layout\n",
    "layout = dict(title = 'Distribution Of {} book-ratings'.format(df.shape[0]),\n",
    "              xaxis = dict(title = 'Rating'),\n",
    "              yaxis = dict(title = 'Count'))\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that over 62% of all ratings in the data are 0, and very few ratings are 1 or 2, or 3, low rating books mean they are generally really bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ratings per book\n",
    "data = df.groupby('ISBN')['bookRating'].count().clip(upper=50)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 50,\n",
    "                                  size = 2))\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Distribution Of Number of Ratings Per Book (Clipped at 50)',\n",
    "                   xaxis = dict(title = 'Number of Ratings Per Book'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('ISBN')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the books received less than 5 ratings, and very few books have many ratings, although the most rated book has received 2,502 ratings.\n",
    "\n",
    "### Ratings Distribution By User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of ratings per user\n",
    "data = df.groupby('userID')['bookRating'].count().clip(upper=50)\n",
    "\n",
    "# Create trace\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 50,\n",
    "                                  size = 2))\n",
    "# Create layout\n",
    "layout = go.Layout(title = 'Distribution Of Number of Ratings Per User (Clipped at 50)',\n",
    "                   xaxis = dict(title = 'Ratings Per User'),\n",
    "                   yaxis = dict(title = 'Count'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('userID')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the users gave less than 5 ratings, and very few users gave many ratings, although the most productive user have given 13,602 ratings.\n",
    "\n",
    "I'm sure you have noticed that the above two charts share the same distribution. The number of ratings per movie and the number of ratings per user decay exponentially.\n",
    "\n",
    "To reduce the dimensionality of the dataset, we will filter out rarely rated movies and rarely rating users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_book_ratings = 50\n",
    "filter_books = df['ISBN'].value_counts() > min_book_ratings\n",
    "filter_books = filter_books[filter_books].index.tolist()\n",
    "\n",
    "min_user_ratings = 50\n",
    "filter_users = df['userID'].value_counts() > min_user_ratings\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "df_new = df[(df['ISBN'].isin(filter_books)) & (df['userID'].isin(filter_users))]\n",
    "print('The original data frame shape:\\t{}'.format(df.shape))\n",
    "print('The new data frame shape:\\t{}'.format(df_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise\n",
    "\n",
    "To load a dataset from a pandas dataframe, we will use the load_from_df() method, we will also need a Reader object, and the rating_scale parameter must be specified. The dataframe must have three columns, corresponding to the user ids, the item ids, and the ratings in this order. Each row thus corresponds to a given rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 9))\n",
    "data = Dataset.load_from_df(df_new[['userID', 'ISBN', 'bookRating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Surprise library, we will benchmark the following algorithms\n",
    "\n",
    "### Basic algorithms\n",
    "\n",
    "#### NormalPredictor\n",
    "\n",
    "* NormalPredictor algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal. This is one of the most basic algorithms that do not do much work.\n",
    "\n",
    "#### BaselineOnly\n",
    "\n",
    "* BasiclineOnly algorithm predicts the baseline estimate for given user and item.\n",
    "\n",
    "#### k-NN algorithms\n",
    "\n",
    "##### KNNBasic\n",
    "\n",
    "* KNNBasic is a basic collaborative filtering algorithm.\n",
    "\n",
    "##### KNNWithMeans\n",
    "\n",
    "* KNNWithMeans is basic collaborative filtering algorithm, taking into account the mean ratings of each user.\n",
    "\n",
    "##### KNNWithZScore\n",
    "\n",
    "* KNNWithZScore is a basic collaborative filtering algorithm, taking into account the z-score normalization of each user.\n",
    "\n",
    "##### KNNBaseline\n",
    "\n",
    "* KNNBaseline is a basic collaborative filtering algorithm taking into account a baseline rating.\n",
    "\n",
    "#### Matrix Factorization-based algorithms\n",
    "\n",
    "##### SVD\n",
    "\n",
    "* SVD algorithm is equivalent to Probabilistic Matrix Factorization (http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)\n",
    "\n",
    "##### SVDpp\n",
    "\n",
    "* The SVDpp algorithm is an extension of SVD that takes into account implicit ratings.\n",
    "\n",
    "##### NMF\n",
    "\n",
    "* NMF is a collaborative filtering algorithm based on Non-negative Matrix Factorization. It is very similar with SVD.\n",
    "\n",
    "#### Slope One\n",
    "\n",
    "* Slope One is a straightforward implementation of the SlopeOne algorithm. (https://arxiv.org/abs/cs/0702144)\n",
    "\n",
    "#### Co-clustering\n",
    "\n",
    "* Co-clustering is a collaborative filtering algorithm based on co-clustering (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.6458&rep=rep1&type=pdf)\n",
    "\n",
    "We use rmse as our accuracy metric for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using ALS')\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 5,\n",
    "               'reg_u': 12,\n",
    "               'reg_i': 5\n",
    "               }\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the train_test_split() to sample a trainset and a testset with given sizes, and use the accuracy metric of rmse. Weâ€™ll then use the fit() method which will train the algorithm on the trainset, and the test() method which will return the predictions made from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "predictions = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = algo.trainset\n",
    "print(algo.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect our predictions in details, we are going to build a pandas data frame with all the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Iu(uid):\n",
    "    \"\"\" return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "    \n",
    "def get_Ui(iid):\n",
    "    \"\"\" return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "df['Iu'] = df.uid.apply(get_Iu)\n",
    "df['Ui'] = df.iid.apply(get_Ui)\n",
    "df['err'] = abs(df.est - df.rui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions = df.sort_values(by='err')[:10]\n",
    "worst_predictions = df.sort_values(by='err')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the best predictions, and they are not lucky guesses. Because Ui is anywhere between 26 to 146, they are not really small, meaning that significant number of users have rated the target book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worst predictions look pretty surprise. Let's look in more details of the last one ISBN \"055358264X\", the book was rated by 47 users, user \"26544\" rated 10, our BaselineOnly algorithm predicts 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.loc[df_new['ISBN'] == '055358264X']['bookRating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "df_new.loc[df_new['ISBN'] == '055358264X']['bookRating'].hist()\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('Number of ratings')\n",
    "plt.title('Number of ratings book ISBN 055358264X has received')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, most of the ratings this book received was \"0\", in another word, most of the users in the data rated this book \"0\", only very few users rated \"10\". Same with the other predictions in \"worst predictions\" list. It seems that for each prediction, the users are some kind of outsiders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
