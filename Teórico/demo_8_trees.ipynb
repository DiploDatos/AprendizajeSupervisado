{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión \n",
    "\n",
    "Vamos a utilizar graphviz para visualización de los árboles.\n",
    "\n",
    "Les dejo un [tutorial](https://bobswift.atlassian.net/wiki/spaces/GVIZ/pages/20971549/How+to+install+Graphviz+software) que ayuda en la instalación, especialmente para Windows y Mac OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción a los árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Dientes\":[True,True,True,False,True,True,True,True,True,False],\n",
    "                     \"Pelo\":[True,True,False,True,True,True,False,False,True,False],\n",
    "                     \"Respira\":[True,True,True,True,True,True,False,True,True,True],\n",
    "                     \"Piernas\":[True,True,False,True,True,True,False,False,True,True],\n",
    "                     \"Especie\":[\"Mamifero\",\"Mamifero\",\"Reptil\",\"Mamifero\",\"Mamifero\",\"Mamifero\",\"Reptil\",\"Reptil\",\"Mamifero\",\"Reptil\"]\n",
    "                     }, \n",
    "                    columns=[\"Dientes\",\"Pelo\",\"Respira\",\"Piernas\",\"Especie\"]\n",
    "                    )\n",
    "\n",
    "features = data[[\"Dientes\",\"Pelo\",\"Respira\",\"Piernas\"]]\n",
    "target = data[\"Especie\"]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la base de datos cuenta con 10 animales, sobre los cuales se ha observado si presentan dientes, si tienen pelo, si respiran y si tienen piernas. La variable dependiente en este caso es la especie. Se desea saber si, a partir de las variables registradas, se puede generar un clasificador que determine si un animal es mamífero o reptil. Para esto utilizaremos un árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación\n",
    "\n",
    "La implementación en python es sencilla. Primero, lo implementaremos con scikit-learn y luego iremos entendiendo qué está haciendo el método de este paquete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion = 'entropy')\n",
    "\n",
    "model.fit(features,target)\n",
    "\n",
    "print(\"The prediction accuracy is: \",model.score(features,target)*100,\"%\")\n",
    "#prediction = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "#tree.plot_tree?\n",
    "tree.plot_tree(model,feature_names=[\"Dientes\",\"Pelo\",\"Respira\",\"Piernas\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora entendamos un poco\n",
    "\n",
    "\n",
    "Entropía $H$:\n",
    "\n",
    "Medida de impureza de un conjunto de datos. La ganancia de información se refleja en un decrecimiento en la medida de entropía.\n",
    "\n",
    "Definición: \n",
    "\n",
    "$H(Q_m) = - \\sum_{k \\in target} P_{mk} log_2(P_{mk})$,\n",
    "\n",
    "donde $Q_m$ son los datos en el nodo $m$ del árbol, la suma se extiende sobre los posibles valores $k$ de la variable respuesta y $P_{mk}$ es la probabilidad condicional que la variable respuesta tome el valor $k$ dado a que estamos en el nodo $m$\n",
    "\n",
    "Caso Gini: $H(Q_m) = \\sum_{k \\in target} P_{mk} (1-P_{mk}))$\n",
    "\n",
    "[Documentación](https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia(P):\n",
    "    '''\n",
    "    Función que calcula medida de entropía dada una probabilidad P.\n",
    "    '''\n",
    "    entropy = - P * np.log2(P)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar la definición de entropía debemos tener una función que estime la probabilidad con la frecuencia relativa.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob(data):\n",
    "    '''\n",
    "    Función que calcula la probabilidad de cada clase.\n",
    "    data : columna categórica de un dataframe de pandas.\n",
    "    '''\n",
    "    return data.value_counts()/data.shape[0]\n",
    "probabilidades = calc_prob(data['Especie'])\n",
    "probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, podemos calcular la entropía total de los datos en el primer nodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropia_total = np.sum([entropia(pi) for pi in probabilidades])\n",
    "print(f'La entropia total es {round(entropia_total,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que este valor coincide con el que declara el método `DecisionTreeClassifier` de `scikit-learn` en el árbol de decisión que obtuvimos anteriormente.\n",
    "\n",
    "\n",
    "Ahora, **¿Por qué eligió la variable pelo para ramificar? ¿Cómo lo hizo?**\n",
    "\n",
    "Para responder esto, calculemos la ganancia de entropía para cada variable descriptiva. Esto lo podemos pensar como:\n",
    "\n",
    "$GananciaEntropia(variable_d) = Entropia_{total} - Entropia(variable_d)$\n",
    "\n",
    "Lo que se puede escribir como:\n",
    "\n",
    "$GananciaEntropia(variable_d) = Entropia_{total} - \\sum_{t \\in variable_d} P(variable_d=t) * H(variable_d=t) $\n",
    "\n",
    "que es igual a:\n",
    "\n",
    "$GananciaEntropia(variable_d) = Entropia_{total} - \\sum_{t \\in variable_d} P(variable_d=t) * (-\\sum_{k \\in target} P(target=k \\cap variable_d = t)) * \\log_2(P((target=k \\cap variable_d = t)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.Dientes == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.Dientes == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probabilidades de ser mamífero o reptil dado que tiene dientes\n",
    "P_especie_dientes = calc_prob(data[data.Dientes == 1]['Especie'])\n",
    "#Probabilidades de ser mamífero o reptil dado que no tiene dientes\n",
    "P_especie_nodientes = calc_prob(data[data.Dientes == 0]['Especie'])\n",
    "#Probabilidad de tener dientes\n",
    "P_dientes = calc_prob(data['Dientes'])\n",
    "\n",
    "#entropias\n",
    "entropia_dientes = P_dientes[1] * (entropia(P_especie_dientes[0]) + entropia(P_especie_dientes[1])) \\\n",
    "                +  P_dientes[0] * (entropia(P_especie_nodientes[0]) + entropia(P_especie_nodientes[1]))\n",
    "\n",
    "entropia_dientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#información ganada\n",
    "entropia_total - entropia_dientes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ganancia_de_info(var,data=data,respuesta='Especie'):\n",
    "    '''\n",
    "    Función que calcula la ganancia de información utilizando la entropía como medida\n",
    "    de información.\n",
    "    \n",
    "    variables de entrada:\n",
    "    var (str): nombre de la variable sobre la cuál vamos a calcular la ganancia de la información.\n",
    "    data (dataFrame): el conjunto de datos de donde sacar la varianza\n",
    "    respuesta (str): nombre de la variable respuesta\n",
    "    \n",
    "    Devuelve la ganancia de información (float)\n",
    "    '''\n",
    "    probabilidades = calc_prob(data[respuesta])\n",
    "    entropia_total = np.sum([entropia(pi) for pi in probabilidades])\n",
    "    P_especie_var = calc_prob(data[respuesta][data[var] == 1])\n",
    "    P_especie_novar = calc_prob(data[respuesta][data[var] == 0])\n",
    "    P_var = calc_prob(data[var])\n",
    "    \n",
    "    entropia_var = P_var[1] * np.sum([entropia(pi) for pi in P_especie_var]) \\\n",
    "                    +  P_var[0] * np.sum([entropia(pi) for pi in P_especie_novar])\n",
    "    return entropia_total - entropia_var\n",
    "\n",
    "print(f'ganancia de información variable dientes: {ganancia_de_info(\"Dientes\"):.3f}')\n",
    "print(f'ganancia de información variable pelo: {ganancia_de_info(\"Pelo\"):.3f}')\n",
    "print(f'ganancia de información variable respira: {ganancia_de_info(\"Respira\"):.3f}')\n",
    "print(f'ganancia de información variable piernas: {ganancia_de_info(\"Piernas\"):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recordemos que las variables son [\"Dientes\",\"Pelo\",\"Respira\",\"Piernas\"]\n",
    "animal_raro_1 = [0,0,0,0]\n",
    "animal_raro_2 = [1,1,0,0]\n",
    "model.predict([animal_raro_1,animal_raro_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso un poco más complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos los datos\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "#armamos dataframe para visualizar\n",
    "df = pd.DataFrame(X,columns=iris.feature_names)\n",
    "df['Especie'] = y\n",
    "df['Especie'].replace({0:'setosa', 1:'versicolor', 2:'virginica'},\n",
    "                      inplace=True)\n",
    "\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regiones de decisión\n",
    "\n",
    "Para facilitar la comprensión, lo que vamos a hacer es sólo generar un modelo de clasificación teniendo en cuenta únicamente las características del sépalo. Como son dos variables, podremos ver exactamente las regiones de decisión en un gráfico bidimensional (sin tener que fijar las otras variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X[:,:2]\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=7,\n",
    "                                   random_state=1)\n",
    "clf.fit(X1, y)\n",
    "\n",
    "print(\"The prediction accuracy is: \",clf.score(X1,y)*100,\"%\")\n",
    "# Busco los valores máximos y mínimos de las variables de sépalo        \n",
    "x_min, x_max = iris.data[:, 0].min() - 1, iris.data[:, 0].max() + 1\n",
    "y_min, y_max = iris.data[:, 1].min() - 1, iris.data[:, 1].max() + 1\n",
    "\n",
    "# Armo grilla de puntos donde vamos a predecir para armar las regiones\n",
    "xx1, xx2 = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "Z2 = clf.predict(np.c_[xx1.ravel(),xx2.ravel()])\n",
    "Z2 = Z2.reshape(xx1.shape)\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(2,1,1)\n",
    "cmap = ListedColormap([\"orange\",\"mediumspringgreen\", \"darkviolet\"])\n",
    "plt.contourf(xx1, xx2, Z2, alpha=0.4, cmap=cmap, levels=3)\n",
    "\n",
    "#defino los colores necesarios para que quede más bonito y consistente con los colores del árbol.\n",
    "cmap = ListedColormap([\"mediumspringgreen\", \"darkviolet\"])\n",
    "ycolor = []\n",
    "for caso in y:\n",
    "    if caso == 0:\n",
    "        ycolor.append(\"orange\")\n",
    "    elif caso == 1:\n",
    "        ycolor.append(\"mediumspringgreen\")\n",
    "    elif caso == 2:\n",
    "        ycolor.append(\"darkviolet\")\n",
    "    else:\n",
    "        ycolor.append(np.nan)\n",
    "        \n",
    "#Agregamos todos los otros datos, pero más suavecitos. El valor de alpha determina la transparencia.\n",
    "plt.scatter(df['sepal length (cm)'],\n",
    "            df['sepal width (cm)'], c=ycolor,alpha=0.7)\n",
    "#Agregamos formato al primer gráfico\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "tree.plot_tree(clf,\n",
    "               feature_names=iris.feature_names[:2],\n",
    "               filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names[:2],  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de inspeccionar el árbol es el método `export_text` del módulo `tree` de `sklearn` que permite visualizar el árbol de decisión en texto. A partir de inspeccionar el árbol de decisión, podemos entender la forma de las regiones de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tree.export_text(clf,feature_names=iris.feature_names[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso el criterio de regresión suele ser el ECM\n",
    "\n",
    "$H(Q_m)=\\frac{1}{n_m}\\sum_{y\\in Q_m}(y-\\bar{y}_m)^2$\n",
    "\n",
    "[Documentación](https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X_california, y_california = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "california = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(california['DESCR'])  # descripción del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = california.data[:,0] # Usamos la feature MedInc\n",
    "y2 = california.target\n",
    "\n",
    "# Sort X and y by ascending values of X\n",
    "\n",
    "sort_idx = X2.flatten().argsort()\n",
    "X2 = X2[sort_idx].reshape(-1, 1)\n",
    "y2 = y2[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.DecisionTreeRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = tree.DecisionTreeRegressor(max_depth=3, criterion=\"squared_error\")\n",
    "#clf2 = tree.DecisionTreeRegressor(criterion=\"squared_error\")\n",
    "\n",
    "clf2 = clf2.fit(X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qué nos están dando las hojas del árbol en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf2, out_file=None, \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafiquemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(X2, y2, c='steelblue',\n",
    "            edgecolor='white', s=70)\n",
    "plt.plot(X2, clf2.predict(X2),\n",
    "         color='black', lw=2)\n",
    "plt.xlabel('median income in block group [MedInc]')\n",
    "plt.ylabel('Median House Values $1000s [MedHV]')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6b65fc4380ac725e50a330b268a227bbdbe91bddfffbf68e5f7ce9848a2b8d5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
