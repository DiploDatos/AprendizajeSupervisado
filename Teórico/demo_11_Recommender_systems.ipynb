{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistemas de recomendación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv('demo_11_dataset/BX-Users.csv', sep=';', encoding=\"latin-1\")\n",
    "user.columns = ['userID', 'Location', 'Age']\n",
    "rating = pd.read_csv('demo_11_dataset/BX-Book-Ratings.csv', sep=';', encoding=\"latin-1\")\n",
    "rating.columns = ['userID', 'ISBN', 'bookRating']\n",
    "df = pd.merge(user, rating, on='userID', how='inner')\n",
    "df.drop(['Location', 'Age'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la distribución de las valoraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['bookRating'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "p1 = plt.bar(data.index,data.values)\n",
    "\n",
    "for rect1 in p1:\n",
    "    height = rect1.get_height()\n",
    "    plt.annotate( f\"{height/data.values.sum()*100:.2f}%\",(rect1.get_x() + rect1.get_width()/2, height+.05),ha=\"center\",va=\"bottom\",fontsize=8)\n",
    "\n",
    "plt.xticks(np.arange(0,11,1))\n",
    "plt.ylabel('Cantidad de libros')\n",
    "plt.xlabel('Valoración')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo es la distribución de número de valoraciones por libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('ISBN')['bookRating'].count()\n",
    "\n",
    "data.clip(upper=50).hist(bins=50)\n",
    "plt.xlabel('Cantidad de valoraciones por libro')\n",
    "plt.ylabel('Cantidad de libros')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los 10 libros más votados\n",
    "data.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos pensar en la distribución de cantidad de valoraciones por usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby('userID')['bookRating'].count()\n",
    "data.clip(upper=50).hist(bins=50)\n",
    "plt.xlabel('Cantidad de valoraciones por usuario')\n",
    "plt.ylabel('Cantidad de usuarios')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoria de los usuarios dan menos de 10 valoraciones. Cuántas valoraciones dio el usuario más activo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('userID')['bookRating'].count().sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto el número de valoraciones por libro y el número de valoraciones por usuario tienen distribuciones que decaen exponencialmente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar problemas de memoria, vamos a filtrar los libros y los usuarios \"outlayers\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_book_ratings = 50\n",
    "filter_books = df['ISBN'].value_counts() > min_book_ratings\n",
    "filter_books = filter_books[filter_books].index.tolist()\n",
    "\n",
    "min_user_ratings = 50\n",
    "filter_users = df['userID'].value_counts() > min_user_ratings\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "df_new = df[(df['ISBN'].isin(filter_books)) & (df['userID'].isin(filter_users))]\n",
    "print('The original data frame shape:\\t{}'.format(df.shape))\n",
    "print('The new data frame shape:\\t{}'.format(df_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armamos un array booleano que nos indica cuáles ISBN's cumplen con la condición lógica de al menos tener 50 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_book_ratings = 50\n",
    "filter_books = df['ISBN'].value_counts() > min_book_ratings\n",
    "filter_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, nos quedamos sólo con los que los cumplen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_books = filter_books[filter_books].index.tolist()\n",
    "filter_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicion1 = (df['ISBN'].isin(filter_books))\n",
    "df[condicion1]['ISBN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con los usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_user_ratings = 50\n",
    "filter_users = df['userID'].value_counts() > min_user_ratings\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "condicion2 = (df['userID'].isin(filter_users))\n",
    "df_new = df[(condicion2 & condicion1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_new.groupby('userID')['bookRating'].count()\n",
    "data.clip(upper=50).hist(bins=50)\n",
    "plt.xlabel('Cantidad de valoraciones por usuario')\n",
    "plt.ylabel('Cantidad de usuarios')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_new.groupby('ISBN')['bookRating'].count()\n",
    "\n",
    "data.clip(upper=50).hist(bins=50)\n",
    "plt.xlabel('Cantidad de valoraciones por libro')\n",
    "plt.ylabel('Cantidad de libros')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda del mejor modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 10)) # 0 = N/A\n",
    "data = Dataset.load_from_df(df_new[['userID', 'ISBN', 'bookRating']], reader)\n",
    "\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for i, algorithm in enumerate([SVD(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), #SVDpp\n",
    "                 KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]):\n",
    "    # Perform cross validation\n",
    "    print(i)\n",
    "    try:\n",
    "        results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "            # Get results & append algorithm name\n",
    "        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "        tmp = tmp._append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "        benchmark.append(tmp)\n",
    "        print(tmp)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Overview](https://www.kdnuggets.com/2017/08/recommendation-system-algorithms-overview.html) de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos:\n",
    "\n",
    "- NormalPredictor : $\\hat{r}_{ui}$ se muestrea de una distribución gaussiana con media $\\hat{\\mu}$ y desvío $\\hat{\\sigma}^2$ obtenidos por máxima verosimilitud utilizando los datos de entrenamiento.\n",
    "\n",
    "- [BaselineOnly](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration) : $\\hat{r}_{ui} = \\mu + b_u + b_i$\n",
    "\n",
    "- [kNNBasic](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#similarity-measures-configuration) : $\\hat{r}_{ui} = \\frac{\\sum_{v \\in vecinos}sim(u,v) \\cdot r_{vi}}{\\sum_{v \\in vecinos} sim(u,v)}$\n",
    "\n",
    "- [Matrix factorization](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) : $\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T p_u$ (si $\\mu$, $b_u$ y $b_i$ son 0 se llama Probabilistic Matrix Factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
    "surprise_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelo y selección de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con el algoritmo `BaselineOnly()`. Además vamos a utilizar el método [Alternating Least Squares](https://sophwats.github.io/2018-04-05-gentle-als.html) para encontrar los $b_u$ y $b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using ALS')\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 5,\n",
    "               'reg_u': 12,\n",
    "               'reg_i': 5\n",
    "               }\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "predictions = algo.fit(trainset).test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = algo.trainset\n",
    "print(algo.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Iu(uid):\n",
    "    \"\"\" return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "    \n",
    "def get_Ui(iid):\n",
    "    \"\"\" return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "df['Iu'] = df.uid.apply(get_Iu)\n",
    "df['Ui'] = df.iid.apply(get_Ui)\n",
    "df['err'] = abs(df.est - df.rui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar las mejores y peores predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions = df.sort_values(by='err')[:10]\n",
    "worst_predictions = df.sort_values(by='err')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué está pasando con esos casos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = '0590353403'\n",
    "df_new.loc[df_new['ISBN'] == book]['bookRating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[df_new['ISBN'] == book]['bookRating'].hist()\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('Number of ratings')\n",
    "plt.title('Number of ratings book ISBN 055358264X has received')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo extraído de [acá](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Building%20Recommender%20System%20with%20Surprise.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias:\n",
    "\n",
    " * [Documentación de Surprise](https://surpriselib.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks_supervisado",
   "language": "python",
   "name": "notebooks_supervisado"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b65fc4380ac725e50a330b268a227bbdbe91bddfffbf68e5f7ce9848a2b8d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
