{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: Extreme Gradiente Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo es basado en https://www.datacamp.com/community/tutorials/xgboost-in-python. El análisis presentado es interesante, vamos a utilizar el set de datos de los precios de las casas de California y el set de datos sobre diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBRegressor, XGBRFRegressor, XGBClassifier, XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing, load_wine\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar una base de datos para predecir el precio de una casa y otra base de datos para predecir la progresión de diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "california = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diabetes = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "data_california = pd.DataFrame(california.data, columns=california.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos los datos.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_diabetes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los nombres de las columnas no ayudan demasiado, por lo que hay que mirar la descripción del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)\n",
    "print(california.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a separar la variable objetivo de ambos dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diabetes['DIAB'] = diabetes.target\n",
    "data_california['PRICE'] = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, yb = data_diabetes.iloc[:,:-1],data_diabetes.iloc[:,-1]\n",
    "Xc, yc = data_california.iloc[:,:-1],data_california.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a separar en dos subconjuntos (train y test) a ambos dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.2, random_state=42)\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb_train.shape, Xc_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a crear y entrenar el modelo XGBoost. La APIs es similar al de sklearn :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRFRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentos = dict(objective ='reg:squarederror',\n",
    "                         colsample_bytree = 0.3,\n",
    "                         learning_rate = 0.2,\n",
    "                         max_depth = 10, \n",
    "                         alpha = 50, \n",
    "                         n_estimators = 10) #jugar con esto\n",
    "\n",
    "xg_regb = [XGBRegressor(**argumentos), \n",
    "        XGBRFRegressor()]\n",
    "\n",
    "xg_regc = [XGBRegressor(**argumentos),\n",
    "        XGBRFRegressor()]\n",
    "\n",
    "diabetes_set = (xg_regb, Xb_train, yb_train, Xb_test, yb_test)\n",
    "california_set = (xg_regc, Xc_train, yc_train, Xc_test, yc_test)\n",
    "\n",
    "for (dataset_models, X_train, y_train, X_test, y_test) in (diabetes_set, california_set):\n",
    "    for model in dataset_models:\n",
    "        print(model)\n",
    "        model.fit(X_train,y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "        plt.figure(figsize=(15,7))\n",
    "        for i, var in enumerate(X_test):\n",
    "            plt.subplot(2, int(X_test.shape[1]/2) + 1, i+1)\n",
    "            plt.scatter(X_test.loc[:,var],y_test, label='trueval')\n",
    "            plt.scatter(X_test.loc[:,var],preds, label='predicted')\n",
    "            plt.title(var)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_regb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos plotear el árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import to_graphviz\n",
    "to_graphviz(xg_regb[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y para problemas de clasificación?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_wine()\n",
    "\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "FOLDS=4\n",
    "cv = KFold(n_splits=FOLDS, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `objective` es la función objetivo a minimizar. Para problemas de clasificación multiclase usualmente usamos `multi:softmax` debido a que da una \"probabilidad\" para cada clase. \n",
    "\n",
    "Tiene la forma:\n",
    "\n",
    "$\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K{e^{z_j}}}$ para $i=1, \\dots, K$ y $\\mathbf{z} = (z_1, \\dots, z_K) \\in \\mathbb{R}^K $\n",
    "\n",
    "Se puede ver la [documentacion](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters) para otras funciones objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [XGBClassifier(objective = \"multi:softmax\", colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                           max_depth = 5, alpha = 10, n_estimators = 10),\n",
    "        XGBRFClassifier(objective = \"multi:softmax\", colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                           max_depth = 5, alpha = 10, n_estimators = 10),\n",
    "        RandomForestClassifier()]\n",
    "\n",
    "clfs_names = ['XGBC', 'XGBRFC', 'RF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf, name in zip(clfs, clfs_names):\n",
    "    avg_accuracy = 0\n",
    "    print(name)\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X,y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[val_idx], y[val_idx]\n",
    "        clf.fit(X_train,y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        avg_accuracy +=accuracy\n",
    "        print(f\"Acc. fold {fold+1}: {accuracy * 100.0 :.2f}\" % ())\n",
    "        if name == 'XGBC':\n",
    "            ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)\n",
    "    avg_accuracy /= FOLDS\n",
    "    print(f'Avg. accuracy = {avg_accuracy * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * [Wikipedia sobre XGBoost](https://es.wikipedia.org/wiki/XGBoost)\n",
    "  * [Documentación de XGBoost](https://xgboost.readthedocs.io/en/stable/)\n",
    "  * [Gradient Boosting in SciKit Learn](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b65fc4380ac725e50a330b268a227bbdbe91bddfffbf68e5f7ce9848a2b8d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
